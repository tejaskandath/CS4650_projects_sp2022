{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tejaskandath/CS4650_projects_sp2022/blob/master/model/model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wfdb\n",
        "\n",
        "from pandas.core import indexers\n",
        "from matplotlib.text import Annotation\n",
        "\n",
        "\n",
        "from wfdb import rdrecord, rdann, processing\n",
        "import wfdb\n",
        "from wfdb.io.annotation import ann_label_table\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "kRmrUMVYAfiq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3637c97d-cad9-4ea4-d435-1ae0411f4500"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wfdb\n",
            "  Downloading wfdb-4.0.0-py3-none-any.whl (161 kB)\n",
            "\u001b[K     |████████████████████████████████| 161 kB 7.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wfdb) (1.3.5)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.10.1 in /usr/local/lib/python3.7/dist-packages (from wfdb) (1.21.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.8.1 in /usr/local/lib/python3.7/dist-packages (from wfdb) (2.23.0)\n",
            "Requirement already satisfied: scipy<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wfdb) (1.7.3)\n",
            "Requirement already satisfied: SoundFile<0.12.0,>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from wfdb) (0.10.3.post1)\n",
            "Requirement already satisfied: matplotlib<4.0.0,>=3.2.2 in /usr/local/lib/python3.7/dist-packages (from wfdb) (3.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4.0.0,>=3.2.2->wfdb) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4.0.0,>=3.2.2->wfdb) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4.0.0,>=3.2.2->wfdb) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4.0.0,>=3.2.2->wfdb) (3.0.9)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib<4.0.0,>=3.2.2->wfdb) (4.1.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas<2.0.0,>=1.0.0->wfdb) (2022.2.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib<4.0.0,>=3.2.2->wfdb) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.8.1->wfdb) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.8.1->wfdb) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.8.1->wfdb) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.8.1->wfdb) (1.24.3)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.7/dist-packages (from SoundFile<0.12.0,>=0.10.0->wfdb) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0->SoundFile<0.12.0,>=0.10.0->wfdb) (2.21)\n",
            "Installing collected packages: wfdb\n",
            "Successfully installed wfdb-4.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from torch.nn import TransformerEncoder\n",
        "from torch.nn import TransformerEncoderLayer\n",
        "from torch.nn import Embedding\n",
        "import torch\n",
        "import pickle\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "2MqR0jQCn8n9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wfdb.io.show_ann_labels()"
      ],
      "metadata": {
        "id": "2e2lEW3qtlbm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfd4ee64-6214-444f-89c2-a4880b5e0fbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    label_store symbol                                    description\n",
            "0             0                              Not an actual annotation\n",
            "1             1      N                                    Normal beat\n",
            "2             2      L                  Left bundle branch block beat\n",
            "3             3      R                 Right bundle branch block beat\n",
            "4             4      a                Aberrated atrial premature beat\n",
            "5             5      V              Premature ventricular contraction\n",
            "6             6      F          Fusion of ventricular and normal beat\n",
            "7             7      J              Nodal (junctional) premature beat\n",
            "8             8      A                   Atrial premature contraction\n",
            "9             9      S     Premature or ectopic supraventricular beat\n",
            "10           10      E                        Ventricular escape beat\n",
            "11           11      j                 Nodal (junctional) escape beat\n",
            "12           12      /                                     Paced beat\n",
            "13           13      Q                            Unclassifiable beat\n",
            "14           14      ~                          Signal quality change\n",
            "16           16      |                     Isolated QRS-like artifact\n",
            "18           18      s                                      ST change\n",
            "19           19      T                                  T-wave change\n",
            "20           20      *                                        Systole\n",
            "21           21      D                                       Diastole\n",
            "22           22      \"                             Comment annotation\n",
            "23           23      =                         Measurement annotation\n",
            "24           24      p                                    P-wave peak\n",
            "25           25      B              Left or right bundle branch block\n",
            "26           26      ^                      Non-conducted pacer spike\n",
            "27           27      t                                    T-wave peak\n",
            "28           28      +                                  Rhythm change\n",
            "29           29      u                                    U-wave peak\n",
            "30           30      ?                                       Learning\n",
            "31           31      !                       Ventricular flutter wave\n",
            "32           32      [      Start of ventricular flutter/fibrillation\n",
            "33           33      ]        End of ventricular flutter/fibrillation\n",
            "34           34      e                             Atrial escape beat\n",
            "35           35      n                   Supraventricular escape beat\n",
            "36           36      @  Link to external data (aux_note contains URL)\n",
            "37           37      x             Non-conducted P-wave (blocked APB)\n",
            "38           38      f                Fusion of paced and normal beat\n",
            "39           39      (                                 Waveform onset\n",
            "40           40      )                                   Waveform end\n",
            "41           41      r       R-on-T premature ventricular contraction\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a_nums = np.arange(800, 895)\n",
        "bad = [813, 814, 815, 816, 817, 818, 819, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839]\n",
        "a_nums = [x for x in a_nums if x not in bad]\n",
        "a_nums = list(map(str, a_nums))\n",
        "\n",
        "anns = [rdann(x, extension='atr', pn_dir='svdb') for x in a_nums]\n",
        "\n",
        "records = [rdrecord(num, smooth_frames= True, pn_dir='svdb') for num in a_nums]\n",
        "\n",
        "heart_rate = [processing.ann2rr(num, 'atr', \"svdb\", as_array=True) for num in a_nums]"
      ],
      "metadata": {
        "id": "Lm8eDSIs3mRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# acceptable_labels = set(['N', 'L', 'R', 'a', 'V', 'F', 'J', 'S', 'E', 'j', '/', 'e', 'n', 'f'])\n",
        "# temp = ['N', 'L', 'R', 'a', 'V', 'F', 'J', 'S', 'E', 'j', '/', 'e', 'n', 'f']\n",
        "acceptable_labels = set(['V', 'S'])\n",
        "temp = ['V', 'S']\n",
        "label_to_num = {}\n",
        "counts = {}\n",
        "for label in temp:\n",
        "    label_to_num[label] = len(label_to_num)\n",
        "    counts[label] = 0"
      ],
      "metadata": {
        "id": "eUpzbLrv8qIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(a_nums))\n",
        "WINDOW_SIZE = 100\n",
        "x_data = []\n",
        "y_data = []\n",
        "for i in range(len(a_nums)):\n",
        "    ann = anns[i]\n",
        "    indices = [i for i, x in enumerate(ann.symbol) if x in acceptable_labels]\n",
        "    idx = np.array(indices)\n",
        "    record = records[i]\n",
        "    signals0 = np.nan_to_num(record.p_signal[:, 0])\n",
        "    for ind in idx:\n",
        "        label_ind = ann.sample[ind]\n",
        "        temp = signals0[(label_ind-int(WINDOW_SIZE/2)):(label_ind+int(WINDOW_SIZE/2))]\n",
        "        if len(temp) != WINDOW_SIZE:\n",
        "            continue\n",
        "        x_data.append(temp)\n",
        "        y_data.append(label_to_num[ann.symbol[ind]])\n",
        "        counts[ann.symbol[ind]] += 1\n",
        "        # print(ann.symbol[i], x)\n",
        "        # plt.plot(signals0[(x-int(WINDOW_SIZE/2)):(x+int(WINDOW_SIZE/2))])\n",
        "        # plt.show()\n",
        "x = torch.tensor(np.array(x_data, dtype=np.float16)).to(\"cuda\")\n",
        "x = x.float()\n",
        "x -= x.min()\n",
        "x /= x.max()\n",
        "y = torch.tensor(np.array(y_data)).to(\"cuda\")\n",
        "print(x.shape, y.shape, x.max(), x.min())\n",
        "for label in counts:\n",
        "    print(label, counts[label]/len(y))"
      ],
      "metadata": {
        "id": "YgfT1ig3v3dX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1665f734-1aee-4ed0-d2a1-da1892e20979"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "78\n",
            "torch.Size([22127, 100]) torch.Size([22127]) tensor(1., device='cuda:0') tensor(0., device='cuda:0')\n",
            "V 0.4492701224748045\n",
            "S 0.5507298775251954\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ETAIP60CKxDM"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_bXcvPcRD9wh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shuffle_order = torch.randperm(len(x))\n",
        "shuffle_order = shuffle_order.tolist()\n",
        "x = x[shuffle_order]\n",
        "y = y[shuffle_order]\n",
        "\n",
        "train_ind = int(len(x) * .85)\n",
        "val_ind = int(len(x) * .95)\n",
        "train_x = x[:train_ind].to(\"cuda\")\n",
        "train_y = y[:train_ind].to(\"cuda\")\n",
        "val_x = x[train_ind:val_ind].to(\"cuda\")\n",
        "val_y = y[train_ind:val_ind].to(\"cuda\")\n",
        "test_x = x[val_ind:].to(\"cuda\")\n",
        "test_y = y[val_ind:].to(\"cuda\")\n",
        "print(train_x.shape, val_x.shape, test_x.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DSWliM7vFMfh",
        "outputId": "c275b4df-bd3a-4820-e6e5-6da804f6c17f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([18807, 100]) torch.Size([2213, 100]) torch.Size([1107, 100])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epoch_count = 300\n",
        "batch_size = 64\n",
        "import time\n",
        "import sys\n",
        "\n",
        "from tqdm import tqdm\n",
        "from time import sleep\n",
        "\n",
        "PATIENCE = 3\n",
        "def train():\n",
        "    patience = PATIENCE\n",
        "    best_val_loss = 1e10\n",
        "    for epoch in range(epoch_count):\n",
        "        print(\"epoch\", epoch)\n",
        "        model.train()\n",
        "        batch_count = len(train_x)//batch_size\n",
        "        epoch_loss = 0\n",
        "        epoch_hits = 0\n",
        "        epoch_data_points = 0\n",
        "        batches = range(batch_count)\n",
        "        total_correct = 0\n",
        "        train_shuffle_order = torch.randperm(len(train_x))\n",
        "        train_shuffle_order = train_shuffle_order.tolist()\n",
        "        epoch_x = train_x[train_shuffle_order]\n",
        "        epoch_y = train_y[train_shuffle_order]\n",
        "        with tqdm(total=batch_count, file=sys.stdout) as pbar:\n",
        "            for batch in batches:\n",
        "                x_batch = epoch_x[batch_size*batch:batch_size*(batch+1)]\n",
        "                y_batch = epoch_y[batch_size*batch:batch_size*(batch+1)]\n",
        "                epoch_data_points += len(x_batch)\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(x_batch)\n",
        "                loss = criterion(outputs, y_batch)\n",
        "                epoch_loss += loss.item()\n",
        "                loss.backward()\n",
        "                loss.detach()\n",
        "                y_out = torch.argmax(outputs.detach(), axis=1)\n",
        "                correct = (y_out == y_batch).sum()\n",
        "                total_correct += correct\n",
        "                optimizer.step()\n",
        "                to_print = \"epoch_loss: {loss}, epoch_accuracy: {acc}\".format(\n",
        "                    loss=\"%.2f\" % (epoch_loss/(batch + 1)), acc=\"%.2f\" % (100 * total_correct / (batch_size * (batch + 1))))\n",
        "                pbar.set_description(to_print)\n",
        "                pbar.update(1)\n",
        "        print(\"final epoch_loss\", \"%.2f\" % (epoch_loss/(batch + 1)), \"final epoch_acc\", \"%.2f\" % (100 * total_correct / len(train_x)))\n",
        "        val_loss = val()\n",
        "        if val_loss > best_val_loss:\n",
        "            patience -= 1\n",
        "        else:\n",
        "            best_val_loss = val_loss\n",
        "            patience = PATIENCE\n",
        "        print(best_val_loss, \"patience\", patience)\n",
        "        if patience < 0:\n",
        "            break\n",
        "        \n",
        "def test():\n",
        "    model.eval()\n",
        "    batch_count = len(test_x)//batch_size\n",
        "    epoch_loss = 0\n",
        "    epoch_hits = 0\n",
        "    epoch_data_points = 0\n",
        "    total_correct = 0\n",
        "    for batch in range(batch_count):\n",
        "        x_batch = test_x[batch_size*batch:batch_size*(batch+1)]\n",
        "        y_batch = test_y[batch_size*batch:batch_size*(batch+1)]\n",
        "        epoch_data_points += len(x_batch)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(x_batch)\n",
        "        y_out = torch.argmax(outputs.detach(), axis=1)\n",
        "        correct = (y_out == y_batch).sum()\n",
        "        total_correct += correct\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        epoch_loss += loss.item()\n",
        "    print(\"test_loss\", \"%.2f\" % (epoch_loss/(batch + 1)), \"test_acc\", \"%.2f\" % (100 * total_correct / len(test_x)))\n",
        "\n",
        "def val():\n",
        "    model.eval()\n",
        "    batch_count = len(val_x)//batch_size\n",
        "    epoch_loss = 0\n",
        "    epoch_hits = 0\n",
        "    epoch_data_points = 0\n",
        "    total_correct = 0\n",
        "    for batch in range(batch_count):\n",
        "        x_batch = val_x[batch_size*batch:batch_size*(batch+1)]\n",
        "        y_batch = val_y[batch_size*batch:batch_size*(batch+1)]\n",
        "        epoch_data_points += len(x_batch)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(x_batch)\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        epoch_loss += loss.item()\n",
        "        y_out = torch.argmax(outputs.detach(), axis=1)\n",
        "        correct = (y_out == y_batch).sum()\n",
        "        total_correct += correct\n",
        "    print(\"val_loss\", \"%.2f\" % (epoch_loss/(batch + 1)), \"val_acc\", \"%.2f\" % (100 * total_correct / len(val_x)))\n",
        "    return (epoch_loss/(batch + 1))\n",
        "\n",
        "train()\n",
        "test()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2x2Tx1JoIPqQ",
        "outputId": "2409b87e-7f4c-43c0-cb9a-974fd2c2261e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0\n",
            "epoch_loss: 0.68, epoch_accuracy: 56.73: 100%|██████████| 293/293 [00:07<00:00, 38.58it/s]\n",
            "final epoch_loss 0.68 final epoch_acc 56.56\n",
            "val_loss 0.67 val_acc 55.99\n",
            "0.6652491741320666 patience 3\n",
            "epoch 1\n",
            "epoch_loss: 0.63, epoch_accuracy: 66.22: 100%|██████████| 293/293 [00:05<00:00, 56.77it/s]\n",
            "final epoch_loss 0.63 final epoch_acc 66.02\n",
            "val_loss 0.55 val_acc 68.46\n",
            "0.5501136884969824 patience 3\n",
            "epoch 2\n",
            "epoch_loss: 0.50, epoch_accuracy: 76.77: 100%|██████████| 293/293 [00:05<00:00, 56.97it/s]\n",
            "final epoch_loss 0.50 final epoch_acc 76.54\n",
            "val_loss 0.38 val_acc 82.38\n",
            "0.3839172256343505 patience 3\n",
            "epoch 3\n",
            "epoch_loss: 0.43, epoch_accuracy: 80.32: 100%|██████████| 293/293 [00:05<00:00, 56.96it/s]\n",
            "final epoch_loss 0.43 final epoch_acc 80.09\n",
            "val_loss 0.40 val_acc 81.16\n",
            "0.3839172256343505 patience 2\n",
            "epoch 4\n",
            "epoch_loss: 0.40, epoch_accuracy: 82.02: 100%|██████████| 293/293 [00:05<00:00, 56.91it/s]\n",
            "final epoch_loss 0.40 final epoch_acc 81.78\n",
            "val_loss 0.37 val_acc 83.24\n",
            "0.3747382632949773 patience 3\n",
            "epoch 5\n",
            "epoch_loss: 0.38, epoch_accuracy: 83.04: 100%|██████████| 293/293 [00:05<00:00, 56.90it/s]\n",
            "final epoch_loss 0.38 final epoch_acc 82.80\n",
            "val_loss 0.40 val_acc 82.06\n",
            "0.3747382632949773 patience 2\n",
            "epoch 6\n",
            "epoch_loss: 0.37, epoch_accuracy: 84.21: 100%|██████████| 293/293 [00:05<00:00, 56.74it/s]\n",
            "final epoch_loss 0.37 final epoch_acc 83.96\n",
            "val_loss 0.37 val_acc 83.55\n",
            "0.3720608568366836 patience 3\n",
            "epoch 7\n",
            "epoch_loss: 0.36, epoch_accuracy: 84.94: 100%|██████████| 293/293 [00:05<00:00, 56.73it/s]\n",
            "final epoch_loss 0.36 final epoch_acc 84.69\n",
            "val_loss 0.36 val_acc 84.18\n",
            "0.3614869556006263 patience 3\n",
            "epoch 8\n",
            "epoch_loss: 0.35, epoch_accuracy: 85.30: 100%|██████████| 293/293 [00:05<00:00, 57.18it/s]\n",
            "final epoch_loss 0.35 final epoch_acc 85.05\n",
            "val_loss 0.35 val_acc 84.91\n",
            "0.3532297611236572 patience 3\n",
            "epoch 9\n",
            "epoch_loss: 0.34, epoch_accuracy: 85.83: 100%|██████████| 293/293 [00:05<00:00, 56.85it/s]\n",
            "final epoch_loss 0.34 final epoch_acc 85.58\n",
            "val_loss 0.38 val_acc 84.46\n",
            "0.3532297611236572 patience 2\n",
            "epoch 10\n",
            "epoch_loss: 0.33, epoch_accuracy: 86.41: 100%|██████████| 293/293 [00:05<00:00, 56.90it/s]\n",
            "final epoch_loss 0.33 final epoch_acc 86.16\n",
            "val_loss 0.33 val_acc 85.99\n",
            "0.32646403242559996 patience 3\n",
            "epoch 11\n",
            "epoch_loss: 0.32, epoch_accuracy: 87.11: 100%|██████████| 293/293 [00:05<00:00, 55.93it/s]\n",
            "final epoch_loss 0.32 final epoch_acc 86.86\n",
            "val_loss 0.30 val_acc 86.35\n",
            "0.30076396027032065 patience 3\n",
            "epoch 12\n",
            "epoch_loss: 0.31, epoch_accuracy: 87.19: 100%|██████████| 293/293 [00:05<00:00, 57.23it/s]\n",
            "final epoch_loss 0.31 final epoch_acc 86.94\n",
            "val_loss 0.30 val_acc 86.44\n",
            "0.30076396027032065 patience 2\n",
            "epoch 13\n",
            "epoch_loss: 0.30, epoch_accuracy: 87.87: 100%|██████████| 293/293 [00:05<00:00, 57.34it/s]\n",
            "final epoch_loss 0.30 final epoch_acc 87.62\n",
            "val_loss 0.30 val_acc 86.85\n",
            "0.29555361498804655 patience 3\n",
            "epoch 14\n",
            "epoch_loss: 0.29, epoch_accuracy: 88.04: 100%|██████████| 293/293 [00:05<00:00, 56.82it/s]\n",
            "final epoch_loss 0.29 final epoch_acc 87.78\n",
            "val_loss 0.29 val_acc 86.90\n",
            "0.2939514026922338 patience 3\n",
            "epoch 15\n",
            "epoch_loss: 0.29, epoch_accuracy: 88.17: 100%|██████████| 293/293 [00:05<00:00, 56.08it/s]\n",
            "final epoch_loss 0.29 final epoch_acc 87.91\n",
            "val_loss 0.27 val_acc 87.30\n",
            "0.2739545588984209 patience 3\n",
            "epoch 16\n",
            "epoch_loss: 0.29, epoch_accuracy: 88.36: 100%|██████████| 293/293 [00:05<00:00, 56.22it/s]\n",
            "final epoch_loss 0.29 final epoch_acc 88.11\n",
            "val_loss 0.28 val_acc 87.62\n",
            "0.2739545588984209 patience 2\n",
            "epoch 17\n",
            "epoch_loss: 0.28, epoch_accuracy: 88.54: 100%|██████████| 293/293 [00:05<00:00, 54.88it/s]\n",
            "final epoch_loss 0.28 final epoch_acc 88.28\n",
            "val_loss 0.27 val_acc 87.53\n",
            "0.2689318152911523 patience 3\n",
            "epoch 18\n",
            "epoch_loss: 0.28, epoch_accuracy: 88.81: 100%|██████████| 293/293 [00:05<00:00, 55.73it/s]\n",
            "final epoch_loss 0.28 final epoch_acc 88.55\n",
            "val_loss 0.28 val_acc 87.84\n",
            "0.2689318152911523 patience 2\n",
            "epoch 19\n",
            "epoch_loss: 0.28, epoch_accuracy: 88.92: 100%|██████████| 293/293 [00:05<00:00, 56.99it/s]\n",
            "final epoch_loss 0.28 final epoch_acc 88.66\n",
            "val_loss 0.26 val_acc 88.03\n",
            "0.26488835452234044 patience 3\n",
            "epoch 20\n",
            "epoch_loss: 0.27, epoch_accuracy: 89.23: 100%|██████████| 293/293 [00:05<00:00, 56.19it/s]\n",
            "final epoch_loss 0.27 final epoch_acc 88.97\n",
            "val_loss 0.26 val_acc 87.84\n",
            "0.264561789439005 patience 3\n",
            "epoch 21\n",
            "epoch_loss: 0.27, epoch_accuracy: 89.35: 100%|██████████| 293/293 [00:05<00:00, 56.45it/s]\n",
            "final epoch_loss 0.27 final epoch_acc 89.08\n",
            "val_loss 0.26 val_acc 88.12\n",
            "0.2573730899568866 patience 3\n",
            "epoch 22\n",
            "epoch_loss: 0.26, epoch_accuracy: 89.52: 100%|██████████| 293/293 [00:05<00:00, 56.03it/s]\n",
            "final epoch_loss 0.26 final epoch_acc 89.26\n",
            "val_loss 0.26 val_acc 88.30\n",
            "0.2573730899568866 patience 2\n",
            "epoch 23\n",
            "epoch_loss: 0.26, epoch_accuracy: 89.54: 100%|██████████| 293/293 [00:05<00:00, 56.24it/s]\n",
            "final epoch_loss 0.26 final epoch_acc 89.28\n",
            "val_loss 0.25 val_acc 88.52\n",
            "0.251789384685895 patience 3\n",
            "epoch 24\n",
            "epoch_loss: 0.26, epoch_accuracy: 89.56: 100%|██████████| 293/293 [00:05<00:00, 56.38it/s]\n",
            "final epoch_loss 0.26 final epoch_acc 89.30\n",
            "val_loss 0.26 val_acc 87.84\n",
            "0.251789384685895 patience 2\n",
            "epoch 25\n",
            "epoch_loss: 0.26, epoch_accuracy: 89.84: 100%|██████████| 293/293 [00:05<00:00, 56.52it/s]\n",
            "final epoch_loss 0.26 final epoch_acc 89.58\n",
            "val_loss 0.25 val_acc 88.79\n",
            "0.24892124687047565 patience 3\n",
            "epoch 26\n",
            "epoch_loss: 0.26, epoch_accuracy: 90.06: 100%|██████████| 293/293 [00:05<00:00, 56.11it/s]\n",
            "final epoch_loss 0.26 final epoch_acc 89.80\n",
            "val_loss 0.27 val_acc 88.34\n",
            "0.24892124687047565 patience 2\n",
            "epoch 27\n",
            "epoch_loss: 0.25, epoch_accuracy: 90.20: 100%|██████████| 293/293 [00:05<00:00, 56.45it/s]\n",
            "final epoch_loss 0.25 final epoch_acc 89.94\n",
            "val_loss 0.25 val_acc 89.15\n",
            "0.24829094331054127 patience 3\n",
            "epoch 28\n",
            "epoch_loss: 0.25, epoch_accuracy: 90.05: 100%|██████████| 293/293 [00:05<00:00, 55.45it/s]\n",
            "final epoch_loss 0.25 final epoch_acc 89.79\n",
            "val_loss 0.25 val_acc 88.39\n",
            "0.24829094331054127 patience 2\n",
            "epoch 29\n",
            "epoch_loss: 0.25, epoch_accuracy: 90.13: 100%|██████████| 293/293 [00:05<00:00, 55.88it/s]\n",
            "final epoch_loss 0.25 final epoch_acc 89.87\n",
            "val_loss 0.25 val_acc 88.66\n",
            "0.24829094331054127 patience 1\n",
            "epoch 30\n",
            "epoch_loss: 0.25, epoch_accuracy: 90.28: 100%|██████████| 293/293 [00:05<00:00, 56.76it/s]\n",
            "final epoch_loss 0.25 final epoch_acc 90.01\n",
            "val_loss 0.25 val_acc 88.79\n",
            "0.24590749140171445 patience 3\n",
            "epoch 31\n",
            "epoch_loss: 0.25, epoch_accuracy: 90.51: 100%|██████████| 293/293 [00:05<00:00, 56.18it/s]\n",
            "final epoch_loss 0.25 final epoch_acc 90.25\n",
            "val_loss 0.24 val_acc 89.38\n",
            "0.24168714376933434 patience 3\n",
            "epoch 32\n",
            "epoch_loss: 0.25, epoch_accuracy: 90.61: 100%|██████████| 293/293 [00:05<00:00, 56.06it/s]\n",
            "final epoch_loss 0.25 final epoch_acc 90.35\n",
            "val_loss 0.24 val_acc 89.61\n",
            "0.236140799434746 patience 3\n",
            "epoch 33\n",
            "epoch_loss: 0.24, epoch_accuracy: 90.67: 100%|██████████| 293/293 [00:05<00:00, 56.35it/s]\n",
            "final epoch_loss 0.24 final epoch_acc 90.40\n",
            "val_loss 0.24 val_acc 89.29\n",
            "0.236140799434746 patience 2\n",
            "epoch 34\n",
            "epoch_loss: 0.24, epoch_accuracy: 90.57: 100%|██████████| 293/293 [00:05<00:00, 55.01it/s]\n",
            "final epoch_loss 0.24 final epoch_acc 90.31\n",
            "val_loss 0.24 val_acc 89.52\n",
            "0.236140799434746 patience 1\n",
            "epoch 35\n",
            "epoch_loss: 0.24, epoch_accuracy: 90.80: 100%|██████████| 293/293 [00:05<00:00, 56.01it/s]\n",
            "final epoch_loss 0.24 final epoch_acc 90.54\n",
            "val_loss 0.23 val_acc 90.10\n",
            "0.23179659137831016 patience 3\n",
            "epoch 36\n",
            "epoch_loss: 0.24, epoch_accuracy: 90.98: 100%|██████████| 293/293 [00:05<00:00, 55.58it/s]\n",
            "final epoch_loss 0.24 final epoch_acc 90.72\n",
            "val_loss 0.23 val_acc 90.06\n",
            "0.23179659137831016 patience 2\n",
            "epoch 37\n",
            "epoch_loss: 0.24, epoch_accuracy: 90.95: 100%|██████████| 293/293 [00:05<00:00, 55.50it/s]\n",
            "final epoch_loss 0.24 final epoch_acc 90.68\n",
            "val_loss 0.23 val_acc 90.01\n",
            "0.22786016411641064 patience 3\n",
            "epoch 38\n",
            "epoch_loss: 0.23, epoch_accuracy: 91.02: 100%|██████████| 293/293 [00:05<00:00, 55.59it/s]\n",
            "final epoch_loss 0.23 final epoch_acc 90.75\n",
            "val_loss 0.23 val_acc 90.06\n",
            "0.22786016411641064 patience 2\n",
            "epoch 39\n",
            "epoch_loss: 0.23, epoch_accuracy: 91.19: 100%|██████████| 293/293 [00:05<00:00, 55.47it/s]\n",
            "final epoch_loss 0.23 final epoch_acc 90.92\n",
            "val_loss 0.23 val_acc 89.92\n",
            "0.22786016411641064 patience 1\n",
            "epoch 40\n",
            "epoch_loss: 0.23, epoch_accuracy: 91.20: 100%|██████████| 293/293 [00:05<00:00, 55.99it/s]\n",
            "final epoch_loss 0.23 final epoch_acc 90.93\n",
            "val_loss 0.23 val_acc 89.79\n",
            "0.22786016411641064 patience 0\n",
            "epoch 41\n",
            "epoch_loss: 0.23, epoch_accuracy: 91.50: 100%|██████████| 293/293 [00:05<00:00, 55.76it/s]\n",
            "final epoch_loss 0.23 final epoch_acc 91.23\n",
            "val_loss 0.22 val_acc 90.56\n",
            "0.22201402835986195 patience 3\n",
            "epoch 42\n",
            "epoch_loss: 0.23, epoch_accuracy: 91.55: 100%|██████████| 293/293 [00:05<00:00, 56.00it/s]\n",
            "final epoch_loss 0.23 final epoch_acc 91.29\n",
            "val_loss 0.24 val_acc 89.15\n",
            "0.22201402835986195 patience 2\n",
            "epoch 43\n",
            "epoch_loss: 0.23, epoch_accuracy: 91.52: 100%|██████████| 293/293 [00:05<00:00, 55.92it/s]\n",
            "final epoch_loss 0.23 final epoch_acc 91.25\n",
            "val_loss 0.22 val_acc 90.51\n",
            "0.2216266663197209 patience 3\n",
            "epoch 44\n",
            "epoch_loss: 0.23, epoch_accuracy: 91.49: 100%|██████████| 293/293 [00:05<00:00, 55.61it/s]\n",
            "final epoch_loss 0.23 final epoch_acc 91.22\n",
            "val_loss 0.22 val_acc 90.10\n",
            "0.2215612324721673 patience 3\n",
            "epoch 45\n",
            "epoch_loss: 0.22, epoch_accuracy: 91.76: 100%|██████████| 293/293 [00:05<00:00, 55.22it/s]\n",
            "final epoch_loss 0.22 final epoch_acc 91.49\n",
            "val_loss 0.21 val_acc 90.96\n",
            "0.2140893136315486 patience 3\n",
            "epoch 46\n",
            "epoch_loss: 0.22, epoch_accuracy: 91.76: 100%|██████████| 293/293 [00:05<00:00, 55.43it/s]\n",
            "final epoch_loss 0.22 final epoch_acc 91.49\n",
            "val_loss 0.22 val_acc 90.65\n",
            "0.2140893136315486 patience 2\n",
            "epoch 47\n",
            "epoch_loss: 0.22, epoch_accuracy: 91.82: 100%|██████████| 293/293 [00:05<00:00, 55.14it/s]\n",
            "final epoch_loss 0.22 final epoch_acc 91.55\n",
            "val_loss 0.21 val_acc 90.87\n",
            "0.2129366417579791 patience 3\n",
            "epoch 48\n",
            "epoch_loss: 0.22, epoch_accuracy: 91.77: 100%|██████████| 293/293 [00:05<00:00, 54.92it/s]\n",
            "final epoch_loss 0.22 final epoch_acc 91.50\n",
            "val_loss 0.21 val_acc 91.01\n",
            "0.21043501093107111 patience 3\n",
            "epoch 49\n",
            "epoch_loss: 0.22, epoch_accuracy: 91.86: 100%|██████████| 293/293 [00:05<00:00, 56.45it/s]\n",
            "final epoch_loss 0.22 final epoch_acc 91.59\n",
            "val_loss 0.21 val_acc 91.01\n",
            "0.20793120541116772 patience 3\n",
            "epoch 50\n",
            "epoch_loss: 0.22, epoch_accuracy: 92.05: 100%|██████████| 293/293 [00:05<00:00, 55.48it/s]\n",
            "final epoch_loss 0.22 final epoch_acc 91.78\n",
            "val_loss 0.21 val_acc 90.87\n",
            "0.20793120541116772 patience 2\n",
            "epoch 51\n",
            "epoch_loss: 0.22, epoch_accuracy: 91.97: 100%|██████████| 293/293 [00:05<00:00, 55.16it/s]\n",
            "final epoch_loss 0.22 final epoch_acc 91.70\n",
            "val_loss 0.21 val_acc 91.05\n",
            "0.20767219693345182 patience 3\n",
            "epoch 52\n",
            "epoch_loss: 0.21, epoch_accuracy: 92.21: 100%|██████████| 293/293 [00:05<00:00, 54.81it/s]\n",
            "final epoch_loss 0.21 final epoch_acc 91.94\n",
            "val_loss 0.21 val_acc 91.01\n",
            "0.20767219693345182 patience 2\n",
            "epoch 53\n",
            "epoch_loss: 0.21, epoch_accuracy: 92.13: 100%|██████████| 293/293 [00:05<00:00, 54.98it/s]\n",
            "final epoch_loss 0.21 final epoch_acc 91.86\n",
            "val_loss 0.20 val_acc 91.37\n",
            "0.20345127023756504 patience 3\n",
            "epoch 54\n",
            "epoch_loss: 0.21, epoch_accuracy: 92.28: 100%|██████████| 293/293 [00:05<00:00, 55.06it/s]\n",
            "final epoch_loss 0.21 final epoch_acc 92.01\n",
            "val_loss 0.20 val_acc 91.41\n",
            "0.20118997782907067 patience 3\n",
            "epoch 55\n",
            "epoch_loss: 0.21, epoch_accuracy: 92.27: 100%|██████████| 293/293 [00:05<00:00, 55.53it/s]\n",
            "final epoch_loss 0.21 final epoch_acc 92.00\n",
            "val_loss 0.21 val_acc 91.01\n",
            "0.20118997782907067 patience 2\n",
            "epoch 56\n",
            "epoch_loss: 0.21, epoch_accuracy: 92.53: 100%|██████████| 293/293 [00:05<00:00, 55.51it/s]\n",
            "final epoch_loss 0.21 final epoch_acc 92.26\n",
            "val_loss 0.20 val_acc 91.46\n",
            "0.1993410979561946 patience 3\n",
            "epoch 57\n",
            "epoch_loss: 0.20, epoch_accuracy: 92.45: 100%|██████████| 293/293 [00:05<00:00, 55.54it/s]\n",
            "final epoch_loss 0.20 final epoch_acc 92.18\n",
            "val_loss 0.20 val_acc 91.32\n",
            "0.19795306914431207 patience 3\n",
            "epoch 58\n",
            "epoch_loss: 0.20, epoch_accuracy: 92.56: 100%|██████████| 293/293 [00:05<00:00, 55.25it/s]\n",
            "final epoch_loss 0.20 final epoch_acc 92.29\n",
            "val_loss 0.20 val_acc 91.28\n",
            "0.19771713763475418 patience 3\n",
            "epoch 59\n",
            "epoch_loss: 0.21, epoch_accuracy: 92.44: 100%|██████████| 293/293 [00:05<00:00, 55.17it/s]\n",
            "final epoch_loss 0.21 final epoch_acc 92.17\n",
            "val_loss 0.20 val_acc 90.78\n",
            "0.19771713763475418 patience 2\n",
            "epoch 60\n",
            "epoch_loss: 0.20, epoch_accuracy: 92.52: 100%|██████████| 293/293 [00:05<00:00, 55.50it/s]\n",
            "final epoch_loss 0.20 final epoch_acc 92.25\n",
            "val_loss 0.19 val_acc 91.64\n",
            "0.19332408433889642 patience 3\n",
            "epoch 61\n",
            "epoch_loss: 0.20, epoch_accuracy: 92.62: 100%|██████████| 293/293 [00:05<00:00, 55.30it/s]\n",
            "final epoch_loss 0.20 final epoch_acc 92.35\n",
            "val_loss 0.19 val_acc 91.46\n",
            "0.19069235147360494 patience 3\n",
            "epoch 62\n",
            "epoch_loss: 0.20, epoch_accuracy: 92.83: 100%|██████████| 293/293 [00:05<00:00, 55.14it/s]\n",
            "final epoch_loss 0.20 final epoch_acc 92.56\n",
            "val_loss 0.19 val_acc 91.46\n",
            "0.19069235147360494 patience 2\n",
            "epoch 63\n",
            "epoch_loss: 0.20, epoch_accuracy: 92.73: 100%|██████████| 293/293 [00:05<00:00, 55.29it/s]\n",
            "final epoch_loss 0.20 final epoch_acc 92.45\n",
            "val_loss 0.19 val_acc 91.64\n",
            "0.19069235147360494 patience 1\n",
            "epoch 64\n",
            "epoch_loss: 0.20, epoch_accuracy: 92.81: 100%|██████████| 293/293 [00:05<00:00, 55.35it/s]\n",
            "final epoch_loss 0.20 final epoch_acc 92.53\n",
            "val_loss 0.19 val_acc 91.73\n",
            "0.18816147525520885 patience 3\n",
            "epoch 65\n",
            "epoch_loss: 0.20, epoch_accuracy: 92.81: 100%|██████████| 293/293 [00:05<00:00, 55.34it/s]\n",
            "final epoch_loss 0.20 final epoch_acc 92.54\n",
            "val_loss 0.19 val_acc 91.69\n",
            "0.18669984358198502 patience 3\n",
            "epoch 66\n",
            "epoch_loss: 0.20, epoch_accuracy: 92.78: 100%|██████████| 293/293 [00:05<00:00, 55.10it/s]\n",
            "final epoch_loss 0.20 final epoch_acc 92.51\n",
            "val_loss 0.19 val_acc 91.78\n",
            "0.18636805454597755 patience 3\n",
            "epoch 67\n",
            "epoch_loss: 0.19, epoch_accuracy: 93.07: 100%|██████████| 293/293 [00:05<00:00, 55.44it/s]\n",
            "final epoch_loss 0.19 final epoch_acc 92.80\n",
            "val_loss 0.19 val_acc 91.50\n",
            "0.18636805454597755 patience 2\n",
            "epoch 68\n",
            "epoch_loss: 0.19, epoch_accuracy: 92.89: 100%|██████████| 293/293 [00:05<00:00, 54.31it/s]\n",
            "final epoch_loss 0.19 final epoch_acc 92.61\n",
            "val_loss 0.18 val_acc 92.09\n",
            "0.18291622082538464 patience 3\n",
            "epoch 69\n",
            "epoch_loss: 0.19, epoch_accuracy: 93.01: 100%|██████████| 293/293 [00:05<00:00, 55.45it/s]\n",
            "final epoch_loss 0.19 final epoch_acc 92.74\n",
            "val_loss 0.18 val_acc 91.60\n",
            "0.18264895820004098 patience 3\n",
            "epoch 70\n",
            "epoch_loss: 0.19, epoch_accuracy: 93.13: 100%|██████████| 293/293 [00:05<00:00, 55.19it/s]\n",
            "final epoch_loss 0.19 final epoch_acc 92.86\n",
            "val_loss 0.18 val_acc 91.64\n",
            "0.17964771280393882 patience 3\n",
            "epoch 71\n",
            "epoch_loss: 0.19, epoch_accuracy: 93.01: 100%|██████████| 293/293 [00:05<00:00, 54.67it/s]\n",
            "final epoch_loss 0.19 final epoch_acc 92.74\n",
            "val_loss 0.18 val_acc 92.05\n",
            "0.1783507975804455 patience 3\n",
            "epoch 72\n",
            "epoch_loss: 0.19, epoch_accuracy: 93.06: 100%|██████████| 293/293 [00:05<00:00, 54.61it/s]\n",
            "final epoch_loss 0.19 final epoch_acc 92.79\n",
            "val_loss 0.18 val_acc 92.14\n",
            "0.1772904147339218 patience 3\n",
            "epoch 73\n",
            "epoch_loss: 0.19, epoch_accuracy: 93.23: 100%|██████████| 293/293 [00:05<00:00, 53.88it/s]\n",
            "final epoch_loss 0.19 final epoch_acc 92.96\n",
            "val_loss 0.18 val_acc 92.09\n",
            "0.1772904147339218 patience 2\n",
            "epoch 74\n",
            "epoch_loss: 0.19, epoch_accuracy: 93.25: 100%|██████████| 293/293 [00:05<00:00, 54.40it/s]\n",
            "final epoch_loss 0.19 final epoch_acc 92.98\n",
            "val_loss 0.18 val_acc 92.00\n",
            "0.1772904147339218 patience 1\n",
            "epoch 75\n",
            "epoch_loss: 0.19, epoch_accuracy: 93.25: 100%|██████████| 293/293 [00:05<00:00, 54.46it/s]\n",
            "final epoch_loss 0.19 final epoch_acc 92.98\n",
            "val_loss 0.18 val_acc 92.09\n",
            "0.1772904147339218 patience 0\n",
            "epoch 76\n",
            "epoch_loss: 0.19, epoch_accuracy: 93.27: 100%|██████████| 293/293 [00:05<00:00, 55.28it/s]\n",
            "final epoch_loss 0.19 final epoch_acc 93.00\n",
            "val_loss 0.17 val_acc 92.18\n",
            "0.17393860670135303 patience 3\n",
            "epoch 77\n",
            "epoch_loss: 0.18, epoch_accuracy: 93.18: 100%|██████████| 293/293 [00:05<00:00, 55.60it/s]\n",
            "final epoch_loss 0.18 final epoch_acc 92.91\n",
            "val_loss 0.18 val_acc 92.27\n",
            "0.17393860670135303 patience 2\n",
            "epoch 78\n",
            "epoch_loss: 0.18, epoch_accuracy: 93.39: 100%|██████████| 293/293 [00:05<00:00, 55.44it/s]\n",
            "final epoch_loss 0.18 final epoch_acc 93.12\n",
            "val_loss 0.18 val_acc 92.14\n",
            "0.17393860670135303 patience 1\n",
            "epoch 79\n",
            "epoch_loss: 0.18, epoch_accuracy: 93.47: 100%|██████████| 293/293 [00:05<00:00, 54.56it/s]\n",
            "final epoch_loss 0.18 final epoch_acc 93.19\n",
            "val_loss 0.17 val_acc 92.14\n",
            "0.169901755912339 patience 3\n",
            "epoch 80\n",
            "epoch_loss: 0.18, epoch_accuracy: 93.36: 100%|██████████| 293/293 [00:05<00:00, 54.91it/s]\n",
            "final epoch_loss 0.18 final epoch_acc 93.08\n",
            "val_loss 0.17 val_acc 92.32\n",
            "0.16941625142798705 patience 3\n",
            "epoch 81\n",
            "epoch_loss: 0.18, epoch_accuracy: 93.44: 100%|██████████| 293/293 [00:05<00:00, 54.60it/s]\n",
            "final epoch_loss 0.18 final epoch_acc 93.17\n",
            "val_loss 0.17 val_acc 92.18\n",
            "0.16941625142798705 patience 2\n",
            "epoch 82\n",
            "epoch_loss: 0.18, epoch_accuracy: 93.66: 100%|██████████| 293/293 [00:05<00:00, 54.17it/s]\n",
            "final epoch_loss 0.18 final epoch_acc 93.39\n",
            "val_loss 0.18 val_acc 91.78\n",
            "0.16941625142798705 patience 1\n",
            "epoch 83\n",
            "epoch_loss: 0.18, epoch_accuracy: 93.63: 100%|██████████| 293/293 [00:05<00:00, 54.27it/s]\n",
            "final epoch_loss 0.18 final epoch_acc 93.36\n",
            "val_loss 0.17 val_acc 92.32\n",
            "0.16941625142798705 patience 0\n",
            "epoch 84\n",
            "epoch_loss: 0.17, epoch_accuracy: 93.64: 100%|██████████| 293/293 [00:05<00:00, 54.24it/s]\n",
            "final epoch_loss 0.17 final epoch_acc 93.36\n",
            "val_loss 0.17 val_acc 92.18\n",
            "0.16941625142798705 patience -1\n",
            "test_loss 0.18 test_acc 91.60\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "id": "CVJDWmdmAXFE",
        "outputId": "99272e0a-975e-4603-f3ab-101725d85665"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ZeroDivisionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-d7b746064205>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnum\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnums\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;31m# if num != \"100\":\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
          ]
        }
      ],
      "source": [
        "sample_freq = 360\n",
        "\n",
        "def getHeartBeat(signal_data):\n",
        "    m = signal_data.mean()\n",
        "    std = signal_data.std()\n",
        "    temp = (signal_data-m)/std\n",
        "    binary = (temp > 2).astype(int)\n",
        "    count = 0\n",
        "    found_peak = False\n",
        "    max_val = -1e10\n",
        "    inds = []\n",
        "    for i, val in enumerate(binary):\n",
        "        if val > 0 and not found_peak:\n",
        "            found_peak = True\n",
        "            max_val = val\n",
        "            inds.append(i)\n",
        "        elif val == 0 and found_peak:\n",
        "            found_peak = False\n",
        "            count += 1\n",
        "    d_inds = []\n",
        "    # print(inds[:10])\n",
        "    for i, val in enumerate(inds[:-1]):\n",
        "        d_inds.append(inds[i+1] - val)\n",
        "    d_inds = np.array(d_inds)\n",
        "    avg_distance = d_inds.mean()\n",
        "    avg_hb = sample_freq * 60/avg_distance\n",
        "    # print(\"average hb per min\", avg_hb)\n",
        "    return inds\n",
        "\n",
        "nums = ['100','101','102','103','104','105','106','107','108','109','111','112','113','114','115','116','117','118',\n",
        "        '119','121','122','123','124','200','201','202','203','205','207','208','209','210','212','213','214','215',\n",
        "        '217','219','220','221','222','223','228','230','231','232','233','234']\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(1/0)\n",
        "for num in nums:\n",
        "    # if num != \"100\":\n",
        "    #     continue\n",
        "    ann = rdann(num, extension='atr', pn_dir='mitdb')\n",
        "    ann_indices = ann.sample\n",
        "    ann_labels = ann.symbol\n",
        "    print(ann_labels[-10:])\n",
        "    count = 0\n",
        "    weird = []\n",
        "    for symbol in ann.symbol:\n",
        "        if symbol != \"N\":\n",
        "            count += 1\n",
        "            weird.append(symbol)\n",
        "    print(weird)\n",
        "    print(count)\n",
        "    print(ann_label_table.columns)\n",
        "    record = rdrecord(num, smooth_frames= True, pn_dir='mitdb')\n",
        "    test = record.p_signal\n",
        "    signals0 = np.nan_to_num(record.p_signal[:,0])\n",
        "    signals1 = np.nan_to_num(record.p_signal[:,1])\n",
        "    print(signals0.shape)\n",
        "    beatInds = getHeartBeat(signals0)\n",
        "    labelInd = 0\n",
        "    trueBeatInds = []\n",
        "    trueBeatLabels = []\n",
        "    dx = []\n",
        "    for ind, beat in enumerate(beatInds):\n",
        "        for i in range(labelInd, len(ann_indices)):\n",
        "            if abs(beat - ann_indices[i]) < 10:\n",
        "                labelInd = i\n",
        "                dx.append(abs(beat - ann_indices[i]))\n",
        "                trueBeatInds.append(ann_indices[i])\n",
        "                trueBeatLabels.append(ann_labels[i])\n",
        "                break\n",
        "    print(sum(dx)/len(dx), labelInd, len(ann_indices))\n",
        "    # plt.plot(signals0[:750])\n",
        "    # plt.show()\n",
        "    # plt.plot(signals1[2044:2402])\n",
        "    # plt.show()\n",
        "\n",
        "    \n",
        "    print(\"\")"
      ]
    }
  ]
}